{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vqmiFd1NjPRd",
        "outputId": "6f226bc3-66eb-4e2c-d7f8-87e9766165d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.4.1.tar.gz (310.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.4.1-py2.py3-none-any.whl size=311285398 sha256=b3dd22dfd7bb6a900a404851a6d4f71852ea51484424a4bc2569481ab5497360\n",
            "  Stored in directory: /root/.cache/pip/wheels/0d/77/a3/ff2f74cc9ab41f8f594dabf0579c2a7c6de920d584206e0834\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.4.1\n"
          ]
        }
      ],
      "source": [
        "%pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "id": "R1G8GzGvja9V",
        "outputId": "3eff497e-b9f3-45ba-88fe-9b4b6c70099f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<SparkContext master=local[*] appName=iic2440>"
            ],
            "text/html": [
              "\n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://d3d7b3d50ca5:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.4.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>iic2440</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "from operator import add, abs\n",
        "\n",
        "\n",
        "spark=SparkSession.builder.config(\"spark.driver.memory\", \"15g\").appName(\"iic2440\").getOrCreate()\n",
        "\n",
        "sc = spark.sparkContext\n",
        "sc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D91Gd4fbK56T"
      },
      "source": [
        "going to assume that nodes always start on 1 and don't skip numbers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Ug-cw_HOrxaN"
      },
      "outputs": [],
      "source": [
        "nodes = [1,2,3,4]\n",
        "edges = [(1,2), (2, 3), (2, 4), (3,2)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "oS9Rs7pLX3IV"
      },
      "outputs": [],
      "source": [
        "def receiver_value(row):\n",
        "  buf = row[1]\n",
        "  receivers = buf[0]\n",
        "  value = buf[1]\n",
        "  nnodes = len(receivers)\n",
        "  value = value/nnodes\n",
        "  for node in receivers:\n",
        "    yield (node, value)\n",
        "\n",
        "#assumes it exist a sparkContect\n",
        "# damping based form https://en.wikipedia.org/wiki/PageRank\n",
        "def page_rank(nodes, edges, damping=0.85, epsilon=0.05, MAXITER=10):\n",
        "  pr = [ (nodes[x], 1/len(nodes)) for x in range(len(nodes))]\n",
        "  rdd_pr = sc.parallelize(pr)\n",
        "  rdd_edges = sc.parallelize(edges)\n",
        "  nnodes = rdd_pr.count()\n",
        "  delta = epsilon + 1\n",
        "  extraValue = ((1-damping) / nnodes)\n",
        "  i = 0\n",
        "  while epsilon < delta and i < MAXITER:\n",
        "    new_ranks = rdd_edges.groupByKey().join(rdd_pr).flatMap(receiver_value).reduceByKey(add).mapValues(lambda rank: (rank * damping) + extraValue)\n",
        "    new_pr = rdd_pr.leftOuterJoin(new_ranks).mapValues(lambda x: x[1] if x[1] is not None else extraValue)\n",
        "    delta = new_pr.join(rdd_pr).map(lambda x: x[1][1] - x[1][0]).map(abs).sum()\n",
        "    rdd_pr = new_pr\n",
        "    i += 1\n",
        "    print('iter', i, ' | delta', delta)\n",
        "  return rdd_pr.collect()\n",
        "\n",
        "def page_rank_non_optimized(nodes, edges, damping=0.85, epsilon=0.05, MAXITER=10):\n",
        "  pr = [ (nodes[x], 1/len(nodes)) for x in range(len(nodes))]\n",
        "  rdd_pr = sc.parallelize(pr)\n",
        "  rdd_edges = sc.parallelize(edges)\n",
        "  nnodes = rdd_pr.count()\n",
        "  delta = epsilon + 1\n",
        "  extraValue = ((1-damping) / nnodes)\n",
        "  i = 0\n",
        "  while epsilon < delta and i < MAXITER:\n",
        "    rdd_node_edge = rdd_edges.groupByKey()\n",
        "    rdd_msgs = rdd_node_edge.join(rdd_pr)\n",
        "    rdd_msgs = rdd_msgs.flatMap(receiver_value)\n",
        "    new_ranks = rdd_msgs.reduceByKey(add).mapValues(lambda rank: (rank * damping) + extraValue)\n",
        "    new_pr = rdd_pr.leftOuterJoin(new_ranks).mapValues(lambda x: x[1] if x[1] is not None else extraValue)\n",
        "    delta = new_pr.join(rdd_pr).map(lambda x: x[1][1] - x[1][0]).map(abs).sum()\n",
        "    rdd_pr = new_pr\n",
        "    i += 1\n",
        "    print('iter', i, ' | delta', delta)\n",
        "  return rdd_pr.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6VDWkShJde5C",
        "outputId": "bc0777a3-6d49-4e8b-cc10-df0d5e45fd25"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter 1  | delta 0.6375\n",
            "iter 2  | delta 0.4515625000000001\n",
            "iter 3  | delta 0.3070625\n",
            "iter 4  | delta 0.16312695312500003\n",
            "iter 5  | delta 0.110926328125\n",
            "iter 6  | delta 0.05892961181640627\n",
            "iter 7  | delta 0.0400721360351563\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(1, 0.037500000000000006),\n",
              " (2, 0.17284380207519534),\n",
              " (3, 0.10670095142822265),\n",
              " (4, 0.10670095142822265)]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "page_rank(nodes, edges)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}